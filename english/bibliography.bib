@inproceedings{he_fastermoe_2022,
  address    = {New York, NY, USA},
  series     = {{PPoPP} '22},
  title      = {{FasterMoE}: modeling and optimizing training of large-scale dynamic pre-trained models},
  isbn       = {978-1-4503-9204-4},
  shorttitle = {{FasterMoE}},
  url        = {https://dl.acm.org/doi/10.1145/3503221.3508418},
  doi        = {10.1145/3503221.3508418},
  abstract   = {The current trend in deep learning is to scale models to extremely large sizes with the objective of increasing their accuracy. Mixture-of-Expert (MoE) is the most popular pre-trained model that makes feasible the training of models with parameters beyond trillion-scale. Thanks to the dynamic activation of experts, i.e., shallow layers specialized in certain domains, it allows for sparse training of bigger models, removing the linearity between model size and computation. However, different from traditional deep learning models, it draws huge challenges to the efficiency of these training systems, including dynamic load imbalance, inefficient synchronous execution mode, and congested all-to-all communication. To address these challenges, we first propose a performance model that can both accurately predict the latency of different operations of a specific training task, and intuitively analyze its end-to-end performance via a novel roofline-like model. Then, guided by this model, we invent a dynamic shadowing approach to cope with load imbalance, and a smart fine-grained schedule that splits different operations and executes them concurrently. We design a congestion-avoiding expert selection strategy that relieves network congestion for the lower latency of iterations, when modification of expert selection is allowed. We implement and integrate the above optimizations as a general system, FasterMoE, empowering efficient distributed MoE model training. FasterMoE is evaluated on different cluster systems using up to 64 GPUs. It achieves 1.37X - 17.87X speedup compared with state-of-the-art systems for large models, including ZeRO, GShard, and BASE Layer. Source code of FasterMoE is now available at https://github.com/thu-pacman/FasterMoE.},
  booktitle  = {Proceedings of the 27th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
  publisher  = {Association for Computing Machinery},
  author     = {He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
  month      = mar,
  year       = {2022},
  keywords   = {distributed deep learning, parallelism, performance modeling},
  pages      = {120--134},
  file       = {Full Text PDF:/home/liblaf/Zotero/storage/AZHZ6ALP/He et al. - 2022 - FasterMoE modeling and optimizing training of lar.pdf:application/pdf}
}
